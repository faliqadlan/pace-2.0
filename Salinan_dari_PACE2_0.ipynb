{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faliqadlan/pace-2.0/blob/main/Salinan_dari_PACE2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZNQlLyaAYGq"
      },
      "outputs": [],
      "source": [
        "# bemd\n",
        "import cupy as cp\n",
        "import gc\n",
        "import logging\n",
        "from cupyx.scipy.ndimage import maximum_filter, minimum_filter, uniform_filter\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "def get_local_extrema(image, window_size=3):\n",
        "    \"\"\"Get local maxima and minima maps for a given image.\"\"\"\n",
        "    # logging.info(f\"Finding local extrema with window size: {window_size}\")\n",
        "    mask = image != 0\n",
        "    max_map = (image == maximum_filter(image, size=window_size)) & mask\n",
        "    min_map = (image == minimum_filter(image, size=window_size)) & mask\n",
        "    return max_map, min_map\n",
        "\n",
        "\n",
        "def apply_order_statistic_filter(image, extrema_map, filter_type=\"max\", window_size=3):\n",
        "    \"\"\"Approximate the envelope using order statistics filters (MAX/MIN)\"\"\"\n",
        "    # logging.info(f\"Applying {filter_type} filter with window size: {window_size}\")\n",
        "    if filter_type == \"max\":\n",
        "        envelope = maximum_filter(image, size=window_size)\n",
        "    elif filter_type == \"min\":\n",
        "        envelope = minimum_filter(image, size=window_size)\n",
        "    else:\n",
        "        raise ValueError(\"filter_type should be either 'max' or 'min'\")\n",
        "    return cp.where(extrema_map, envelope, image)\n",
        "\n",
        "\n",
        "def smooth_envelope(envelope, smooth_window_size=3):\n",
        "    \"\"\"Smooth the envelope with an averaging filter.\"\"\"\n",
        "    # logging.info(f\"Smoothing envelope with window size: {smooth_window_size}\")\n",
        "    return uniform_filter(envelope, size=smooth_window_size)\n",
        "\n",
        "\n",
        "def calculate_mean_envelope(upper_envelope, lower_envelope):\n",
        "    \"\"\"Calculate the mean envelope.\"\"\"\n",
        "    # logging.info(\"Calculating mean envelope\")\n",
        "    return (upper_envelope + lower_envelope) / 2\n",
        "\n",
        "\n",
        "def calculate_standard_deviation(FTj, FTj_next):\n",
        "    \"\"\"Calculate the standard deviation used for BIMF criteria checking.\"\"\"\n",
        "    # logging.info(\"Calculating standard deviation\")\n",
        "    return cp.sqrt(cp.sum((FTj_next - FTj) ** 2) / cp.sum(FTj**2))\n",
        "\n",
        "\n",
        "def fabemd(image, max_iterations=10, threshold=0.2, initial_window_size=3, local_extrema_count=5):\n",
        "    \"\"\"\n",
        "    Perform FABEMD decomposition on an input image.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): Input image (2D array).\n",
        "        max_iterations (int): Maximum iterations for BIMF extraction.\n",
        "        threshold (float): Threshold for standard deviation to accept BIMF.\n",
        "        initial_window_size (int): Initial window size for finding extrema.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of extracted BIMFs.\n",
        "        np.ndarray: Residue of the decomposition.\n",
        "    \"\"\"\n",
        "    #logging.info(\"Starting FABEMD decomposition\")\n",
        "    residual = image.astype(cp.float64)\n",
        "    BIMFs = []\n",
        "    lenBIMFs = len(BIMFs)\n",
        "    limitSD = threshold\n",
        "\n",
        "    # Iterate to extract each BIMF\n",
        "    while True:\n",
        "        FTj = residual.copy()\n",
        "        window_size = initial_window_size\n",
        "        SD = 1.0\n",
        "        for j in range(max_iterations):\n",
        "            #logging.info(f\"Iteration {j+1}/{max_iterations}\")\n",
        "\n",
        "            # Step 1: Find local maxima and minima\n",
        "            max_map, min_map = get_local_extrema(FTj, window_size=window_size)\n",
        "\n",
        "            # Step 2: Estimate upper and lower envelopes using MAX/MIN filters\n",
        "            upper_envelope = apply_order_statistic_filter(\n",
        "                FTj, max_map, filter_type=\"max\", window_size=window_size\n",
        "            )\n",
        "            lower_envelope = apply_order_statistic_filter(\n",
        "                FTj, min_map, filter_type=\"min\", window_size=window_size\n",
        "            )\n",
        "\n",
        "            # Step 3: Smooth the envelopes\n",
        "            upper_envelope = smooth_envelope(\n",
        "                upper_envelope, smooth_window_size=window_size\n",
        "            )\n",
        "            lower_envelope = smooth_envelope(\n",
        "                lower_envelope, smooth_window_size=window_size\n",
        "            )\n",
        "\n",
        "\n",
        "            # Step 4: Calculate the mean envelope\n",
        "            mean_envelope = calculate_mean_envelope(upper_envelope, lower_envelope)\n",
        "            # print(mean_envelope)\n",
        "            # print(lower_envelope)\n",
        "            # Step 5: Update FTj for the next iteration\n",
        "            FTj_next = FTj - mean_envelope\n",
        "            SD = calculate_standard_deviation(FTj, FTj_next)\n",
        "\n",
        "            # Check if the BIMF conditions are met\n",
        "            if SD < limitSD:\n",
        "                # logging.info(f\"BIMF condition met with SD: {SD}\")\n",
        "                BIMFs.append(FTj_next)\n",
        "                residual -= FTj_next\n",
        "                break\n",
        "            else:\n",
        "                limitSD = 1.1 * limitSD\n",
        "                FTj = FTj_next\n",
        "                #window_size += 2  # Optionally adjust window size with each iteration\n",
        "\n",
        "        # Stop if the residual has fewer than 3 extrema points\n",
        "        max_map, min_map = get_local_extrema(residual)\n",
        "        # logging.info(f\"Residual has {cp.sum(max_map)} maxima and {cp.sum(min_map)} minima and {len(BIMFs)} BIMFs, SD = {SD}\")\n",
        "        print(f\"Residual has {cp.sum(max_map)} maxima and {cp.sum(min_map)} minima and {len(BIMFs)} BIMFs, SD = {SD}\", end=\"\\r\")\n",
        "        if ((cp.sum(max_map) + cp.sum(min_map)) <= local_extrema_count):\n",
        "            logging.info(f\"Stopping criteria met: fewer than {local_extrema_count} extrema points\")\n",
        "            break\n",
        "        elif (len(BIMFs) == 100):\n",
        "            logging.info(f\"Stopping criteria met: 100 BIMFs extracted\")\n",
        "            break\n",
        "\n",
        "        del max_map, min_map\n",
        "        gc.collect()\n",
        "    logging.info(\"FABEMD decomposition completed\")\n",
        "\n",
        "    return BIMFs\n",
        "\n",
        "\n",
        "# Usage:\n",
        "# Assuming `input_image` is the (4096, 3255) grayscale image\n",
        "# FABEMD decomposition on an example large image\n",
        "# BIMFs, residue = fabemd(input_image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nonlinier filtering\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "\n",
        "def denoise(bimfs, energies, filtered_residual, R = 1, beta = 0.5):\n",
        "    # Sort BIMFs based on energy\n",
        "    sorted_indices = np.argsort(energies)\n",
        "\n",
        "    # Denoise R components with the lowest energy\n",
        "    denoised_bimfs = []\n",
        "    for i in range(int(R)):\n",
        "        index = sorted_indices[i]\n",
        "        denoised_bimfs.append(cv2.bilateralFilter(bimfs[index].get().astype(np.float32), 5, 75, 75))\n",
        "\n",
        "    # Combine denoised BIMFs and original BIMFs\n",
        "    I_E = np.sum(denoised_bimfs, axis=0)\n",
        "    for j in range(int(R), len(bimfs)):\n",
        "        index = sorted_indices[j]\n",
        "        I_E += np.array(bimfs[index].get())\n",
        "\n",
        "    # Reconstruct the image by adding the filtered residual\n",
        "    I_L = I_E + beta * filtered_residual\n",
        "    return I_L"
      ],
      "metadata": {
        "id": "eJCAWcPTC5Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#homomorphic filter\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import gc\n",
        "\n",
        "def homomorphic_filter(image, d0=30, rh=2.0, rl=0.5, c=1.0):\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        img = image.get()\n",
        "    else:\n",
        "        img = image\n",
        "    rows, cols = img.shape\n",
        "\n",
        "    # Apply logarithmic transform\n",
        "    log_image = np.log1p(img)\n",
        "\n",
        "    # Perform Fourier transform\n",
        "    dft = cv2.dft(log_image, flags=cv2.DFT_COMPLEX_OUTPUT)\n",
        "\n",
        "    dft_shift = np.fft.fftshift(dft)\n",
        "    #print(dft_shift.shape)\n",
        "\n",
        "    # Create high-frequency emphasis filter (HEF)\n",
        "    u = np.arange(cols)\n",
        "    v = np.arange(rows)\n",
        "    u, v = np.meshgrid(u - rows / 2, v - cols / 2)\n",
        "    d = np.sqrt(u**2 + v**2)\n",
        "    h = (rh - rl) * (1 - np.exp(-c * (d**2 / d0**2))) + rl\n",
        "    #print(h.shape)\n",
        "    # Apply filter\n",
        "    h = np.repeat(h[:, :, np.newaxis], 2, axis=2)\n",
        "\n",
        "    dft_shift_filtered = dft_shift * h\n",
        "\n",
        "    # Perform inverse Fourier transform\n",
        "    dft_shift_filtered = np.fft.ifftshift(dft_shift_filtered)\n",
        "    idft = cv2.idft(dft_shift_filtered)\n",
        "    idft = cv2.magnitude(idft[:, :, 0], idft[:, :, 1])\n",
        "\n",
        "    # Normalize the idft result to avoid overflow in expm1\n",
        "    idft = cv2.normalize(idft, None, 0, 1, cv2.NORM_MINMAX)\n",
        "\n",
        "    # Apply exponential transform\n",
        "    exp_image = np.expm1(idft)\n",
        "\n",
        "    # Handle NaN and Inf values\n",
        "    exp_image = np.nan_to_num(exp_image, nan=0.0, posinf=65535.0, neginf=0.0)\n",
        "\n",
        "    # Normalize the image to 0-255\n",
        "    exp_image = cv2.normalize(exp_image, None, 0, 65535, cv2.NORM_MINMAX)\n",
        "    exp_image = np.uint16(exp_image)\n",
        "\n",
        "    del img, rows, cols\n",
        "    del log_image, dft, dft_shift\n",
        "    del u, v, d, h\n",
        "    del dft_shift_filtered, idft\n",
        "    gc.collect()\n",
        "\n",
        "    return exp_image\n"
      ],
      "metadata": {
        "id": "4B2uqRJ8CtWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gamma corection\n",
        "import numpy as np\n",
        "def gamma_correction(image, gamma=0.8):\n",
        "    # Normalize the image to the range [0, 1]\n",
        "    # img_normalized = image / 65535.0\n",
        "\n",
        "    # Apply gamma correction\n",
        "    img_gamma_corrected = np.power(image, gamma)\n",
        "\n",
        "    # Scale back to the 16-bit range [0, 65535]\n",
        "    #img_gamma_corrected = np.uint16(img_gamma_corrected * 65535)\n",
        "\n",
        "    return img_gamma_corrected"
      ],
      "metadata": {
        "id": "RVMmqpP9AyCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image resizer\n",
        "\n",
        "import cupy as cp\n",
        "from  cupyx.scipy.ndimage import zoom\n",
        "import cv2\n",
        "import pathlib as Path\n",
        "\n",
        "\n",
        "def resize(image, new_width):\n",
        "    # Calculate the new dimensions\n",
        "    height, width = image.shape[:2]\n",
        "    width_percent = new_width / float(width)\n",
        "    new_height = int(height * width_percent)\n",
        "\n",
        "    # Resize the image using cupyx.scipy.ndimage.zoom\n",
        "    zoom_factors = (new_height / height, new_width / width, 1) if len(image.shape) == 3 else (new_height / height, new_width / width)\n",
        "    resized_image = zoom(cp.array(image), zoom_factors, order=1)\n",
        "    return resized_image\n",
        "\n",
        "def resize_image(input_image, new_width, input_path=None, output_path=None):\n",
        "    #check if input_image is not None\n",
        "    if input_image is not None:\n",
        "        #check if input_image is a cupy array\n",
        "        if isinstance(input_image, cp.ndarray):\n",
        "            image = input_image\n",
        "        else:\n",
        "            image = cp.array(input_image)\n",
        "        return resize(image, new_width)\n",
        "    else:\n",
        "        # Load the image\n",
        "        image = cv2.imread(input_path, -1)\n",
        "        # Resize the image\n",
        "        resized_image = resize(image, new_width)\n",
        "        # Convert back to numpy array and save the image\n",
        "        resized_image = cp.asnumpy(resized_image)\n",
        "        cv2.imwrite(output_path, resized_image)\n",
        "        out_file = Path(output_path)\n",
        "        if out_file.exists():\n",
        "            return True\n",
        "        else:\n",
        "            return False"
      ],
      "metadata": {
        "id": "npIz1xDZBDRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import cv2\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "from cupyx.scipy.ndimage import median_filter\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "2gVG0SjfBIWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kalibrasi distorsi\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "def get_distortion_parameters(calibration_image_path, parameters_file, pattern_size=(44,35), roi_crop=(0, 0, 4096, 3000)):\n",
        "    # Prepare object points based on the real-world coordinates of the dot matrix points\n",
        "    objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)\n",
        "    objp[:, :2] = np.mgrid[0 : pattern_size[0], 0 : pattern_size[1]].T.reshape(-1, 2)\n",
        "\n",
        "    # Arrays to store object points and image points from all the images\n",
        "    objpoints = []  # 3d point in real world space\n",
        "    imgpoints = []  # 2d points in image plane\n",
        "    img = cv2.imread(calibration_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    ret, centers = cv2.findCirclesGrid(img, pattern_size, cv2.CALIB_CB_SYMMETRIC_GRID)\n",
        "    # If found, add object points, image points (after refining them)\n",
        "    if ret:\n",
        "        objpoints.append(objp)\n",
        "        imgpoints.append(centers)\n",
        "\n",
        "    # Calibrate the camera\n",
        "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\n",
        "    objpoints, imgpoints, img.shape[::-1], None, None\n",
        "    )\n",
        "\n",
        "    np.savez(parameters_file,\n",
        "             mtx=mtx,\n",
        "             dist=dist,\n",
        "             rvecs=rvecs,\n",
        "             tvecs=tvecs,\n",
        "             roi=roi_crop)\n",
        "    param_file = Path(parameters_file)\n",
        "\n",
        "    print('Distortion Parameters')\n",
        "    print('mtx = ',mtx)\n",
        "    print('dsit = ',dist)\n",
        "    print('rvecs = ',rvecs)\n",
        "    print('tvecs = ',tvecs)\n",
        "    print('roi = ',roi_crop)\n",
        "\n",
        "    if param_file.exists():\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# def main():\n",
        "#     if len(sys.argv) < 3:\n",
        "#         print(\"Usage: python kalibrasi_distorsi.py <calibration_image_path> <parameters_file> [row col] [x y w h]\")\n",
        "#         sys.exit(1)\n",
        "#     image_path = sys.argv[1]\n",
        "#     param_file = sys.argv[2]\n",
        "#     pat_size = tuple(sys.argv[3],sys.argv[4]) if len(sys.argv) > 4 else None\n",
        "#     roi = tuple(sys.argv[5],sys.argv[6],sys.argv[7],sys.argv[8]) if len(sys.argv) > 8 else None\n",
        "\n",
        "#     if pat_size is None and roi is None:\n",
        "#         if get_distortion_parameters(calibration_image_path=image_path,\n",
        "#                                  parameters_file=param_file):\n",
        "#             print('Parameters Successfully save to : '+ param_file)\n",
        "#         else:\n",
        "#             print('Error Saving Parameters File')\n",
        "#     elif roi is None:\n",
        "#         if get_distortion_parameters(calibration_image_path=image_path,\n",
        "#                                  parameters_file=param_file, pattern_size=pat_size):\n",
        "#             print('Parameters Successfully save to : '+ param_file)\n",
        "#         else:\n",
        "#             print('Error Saving Parameters File')\n",
        "#     else:\n",
        "#         get_distortion_parameters(calibration_image_path=image_path,\n",
        "#                                  parameters_file=param_file,\n",
        "#                                  pattern_size=pat_size,\n",
        "#                                  roi_crop=roi)\n",
        "#         print('Parameters Successfully save to : '+ param_file)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "l2ITmssWBMAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric\n",
        "\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "\n",
        "\n",
        "def calculate_contrast(image, mask):\n",
        "    \"\"\"\n",
        "    Calculate the contrast of a region in the image defined by the mask.\n",
        "\n",
        "    Parameters:\n",
        "    image (np.ndarray): Grayscale image.\n",
        "    mask (np.ndarray): Binary mask defining the region of interest.\n",
        "\n",
        "    Returns:\n",
        "    float: Contrast value.\n",
        "    \"\"\"\n",
        "    foreground = image[mask == 1]\n",
        "    background = image[mask == 0]\n",
        "\n",
        "    # print(\"fore\", foreground)\n",
        "    # print(\"back\", background)\n",
        "\n",
        "    # if len(foreground) == 0 or len(background) == 0:\n",
        "    #     return 0.0  # Return 0 contrast if there are no valid pixels\n",
        "\n",
        "    X_f = np.mean(foreground)\n",
        "    X_b = np.mean(background)\n",
        "\n",
        "    if len(background) == 0:\n",
        "        X_b = 0\n",
        "\n",
        "    if X_f + X_b == 0:\n",
        "        return 0.0  # Avoid division by zero\n",
        "\n",
        "    contrast = (X_f - X_b) / (X_f + X_b)\n",
        "    return contrast\n",
        "\n",
        "\n",
        "def calculate_cii(processed_image, reference_image, mask):\n",
        "    \"\"\"\n",
        "    Calculate the Contrast Improvement Index (CII).\n",
        "\n",
        "    Parameters:\n",
        "    processed_image (np.ndarray): Processed grayscale image.\n",
        "    reference_image (np.ndarray): Reference (original) grayscale image.\n",
        "    mask (np.ndarray): Binary mask defining the region of interest.\n",
        "\n",
        "    Returns:\n",
        "    float: CII value.\n",
        "    \"\"\"\n",
        "    C_processed = calculate_contrast(processed_image, mask)\n",
        "    C_reference = calculate_contrast(reference_image, mask)\n",
        "\n",
        "    # print(C_processed)\n",
        "    # print(C_reference)\n",
        "\n",
        "    CII = C_processed / C_reference\n",
        "    return CII\n",
        "\n",
        "def calculate_entropy(image):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of an image.\n",
        "\n",
        "    Parameters:\n",
        "    image (np.ndarray): Grayscale image.\n",
        "\n",
        "    Returns:\n",
        "    float: Entropy value.\n",
        "    \"\"\"\n",
        "    # Hitung histogram gambar\n",
        "    hist = cv2.calcHist([image], [0], None, [65535], [0, 65535])\n",
        "\n",
        "    # Normalisasi histogram sehingga jumlahnya menjadi 1\n",
        "    hist = hist / hist.sum()\n",
        "\n",
        "    # Hitung entropy\n",
        "    entropy = -np.sum(\n",
        "        hist * np.log(hist + 1e-7)\n",
        "    )  # Tambahkan 1e-7 untuk menghindari log(0)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def calculate_eme(image, r, c, epsilon=0.0001):\n",
        "    \"\"\"\n",
        "    Calculate the Effective Measure of Enhancement (EME) of an image.\n",
        "\n",
        "    Parameters:\n",
        "    image (np.ndarray): Grayscale image.\n",
        "    r (int): Number of rows to split the image into.\n",
        "    c (int): Number of columns to split the image into.\n",
        "    epsilon (float): Small constant to avoid division by zero.\n",
        "\n",
        "    Returns:\n",
        "    float: EME value.\n",
        "    \"\"\"\n",
        "    height, width = image.shape\n",
        "    block_height = height // r\n",
        "    block_width = width // c\n",
        "\n",
        "    eme = 0.0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            # Extract the block\n",
        "            block = image[\n",
        "                i * block_height : (i + 1) * block_height,\n",
        "                j * block_width : (j + 1) * block_width,\n",
        "            ]\n",
        "\n",
        "            # Calculate the maximum and minimum intensity levels in the block\n",
        "            I_max = np.max(block)\n",
        "            I_min = np.min(block)\n",
        "\n",
        "            if I_min + epsilon == 0:\n",
        "                continue  # Skip this block to avoid division by zero\n",
        "\n",
        "            # Calculate the contrast ratio for the block\n",
        "            CR = I_max / (I_min + epsilon)\n",
        "\n",
        "            # Update the EME value\n",
        "            eme += 20 * np.log(CR)\n",
        "\n",
        "    # Normalize the EME value by the number of blocks\n",
        "    eme /= r * c\n",
        "\n",
        "    return eme\n",
        "\n",
        "\n",
        "# # Contoh penggunaan\n",
        "# # Misalkan kita memiliki gambar yang diproses\n",
        "# processed_image = cv2.imread(\"processed_image.png\", cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# # Hitung EME dengan membagi gambar menjadi 4x4 blok\n",
        "# r, c = 4, 4\n",
        "# eme = calculate_eme(processed_image, r, c)\n",
        "\n",
        "# # Tampilkan hasil\n",
        "# print(\"Effective Measure of Enhancement (EME):\", eme)"
      ],
      "metadata": {
        "id": "1RvzaqggBQsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #json\n",
        "\n",
        "# {\n",
        "#     \"proj_img_path\":\"E:/DataBetaTest/07082024/36-KSW-36B/mentah/36-KSW-36B_Pedis_R 36-KSW-36B 80kV50mA0,50s -8_7_2024-10.34 AM [Administrator].mdn\",\n",
        "#     \"gain_img_path\":\"E:/Project/imager-pipeline/imager-pipeline/datacitra/Gain/Bed/80_50_0,50.mdn\",\n",
        "#     \"dark_img_path\":\"E:/Project/imager-pipeline/imager-pipeline/datacitra/Dark/Bed/dark.mdn\"\n",
        "# }"
      ],
      "metadata": {
        "id": "CreOaGbbBgKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import gc\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# import ffc\n",
        "# import calibrate_image\n",
        "# import bemd\n",
        "\n",
        "# import image_resizer as ir\n",
        "# import homomorphic_filter as hf\n",
        "# import nonlinear_filtering as nf\n",
        "# import metrics\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "def gamma_correction(image, gamma=0.8):\n",
        "    # Normalize the image to the range [0, 1]\n",
        "    img_normalized = image / 65535.0\n",
        "\n",
        "    # Apply gamma correction\n",
        "    img_gamma_corrected = np.power(img_normalized, gamma)\n",
        "\n",
        "    # Scale back to the 16-bit range [0, 65535]\n",
        "    img_gamma_corrected = np.uint16(img_gamma_corrected * 65535)\n",
        "\n",
        "    return img_gamma_corrected\n",
        "\n",
        "def apply_clahe(image, clip_limit=0.5, tile_grid_size=(8, 8)):\n",
        "    # Create a CLAHE object\n",
        "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "\n",
        "    # Apply CLAHE to the image\n",
        "    return clahe.apply(image)\n",
        "\n",
        "def process_parameters(params, reference_image, BIMFs, energies):\n",
        "    d0, rh, rl, gamma, clip_limit, tile_grid_size = params\n",
        "\n",
        "    # Apply homomorphic filter\n",
        "    filtered_image = homomorphic_filter(reference_image, d0=d0, rh=rh, rl=rl)\n",
        "\n",
        "    # Reconstruct the image\n",
        "    reconstructed_image = denoise(BIMFs, energies, filtered_image, R=1, beta=0.5)\n",
        "\n",
        "    # Apply gamma correction\n",
        "    gamma_corrected_image = gamma_correction(reconstructed_image, gamma=gamma)\n",
        "\n",
        "    # Apply CLAHE\n",
        "    clahe_image = apply_clahe(gamma_corrected_image, clip_limit=clip_limit, tile_grid_size=tile_grid_size)\n",
        "\n",
        "    # Calculate CII\n",
        "    mask = np.ones_like(reference_image)\n",
        "    cii = calculate_cii(clahe_image, reference_image, mask)\n",
        "\n",
        "    # Calculate entropy\n",
        "    entropy = calculate_entropy(clahe_image)\n",
        "\n",
        "    # Calculate EME\n",
        "    r, c = 4, 4\n",
        "    eme = calculate_eme(clahe_image, r, c)\n",
        "\n",
        "    evaluation_result = cii + entropy + eme\n",
        "\n",
        "    del filtered_image, reconstructed_image, gamma_corrected_image, mask\n",
        "    gc.collect()\n",
        "\n",
        "    return evaluation_result, cii, entropy, eme, params, clahe_image\n",
        "\n",
        "\n",
        "def load_config(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        config = json.load(file)\n",
        "    return config\n",
        "\n",
        "def set_env_variables(config):\n",
        "    for key, value in config.items():\n",
        "        os.environ[key] = value\n",
        "\n",
        "def get_env_variable(var_name):\n",
        "    return os.getenv(var_name)\n",
        "\n",
        "def main():\n",
        "    logging.info(\"Loading Images..\")\n",
        "    # datacitra_path = os.path.abspath(os.path.join(os.path.dirname(\"main.py\"), os.pardir, 'datacitra'))\n",
        "\n",
        "    # config_path = \"config.json\"\n",
        "\n",
        "    # Load the configuration from the json file\n",
        "    # with open(config_path, 'r') as f:\n",
        "    #     config = json.load(f)\n",
        "\n",
        "    # proj_img_path = datacitra_path + os.sep + '3_WWI_03B_Thorax_AP.tiff'\n",
        "    # gain_img_path = datacitra_path + os.sep + 'Gain' + os.sep + 'Trx' + os.sep + '90_40_0,50.mdn'\n",
        "    # dark_img_path = datacitra_path + os.sep + 'Dark' + os.sep + 'Trx' + os.sep + 'dark.mdn'\n",
        "\n",
        "\n",
        "    proj_img_path = '3_WWI_03B_Thorax_AP.tiff'\n",
        "    gain_img_path = '90_40_0,50.mdn'\n",
        "    dark_img_path = 'dark.mdn'\n",
        "\n",
        "    # # Extract the paths from the configuration\n",
        "    # proj_img_path = config.get('proj_img_path')\n",
        "    # gain_img_path = config.get('gain_img_path')\n",
        "    # dark_img_path = config.get('dark_img_path')\n",
        "\n",
        "    # print(\"Project Image Path:\", proj_img_path)\n",
        "    # print(\"Gain Image Path:\", gain_img_path)\n",
        "    # print(\"Dark Image Path:\", dark_img_path)\n",
        "\n",
        "    proj_img = cv2.imread(proj_img_path, -1)\n",
        "    gain_img = cv2.imread(gain_img_path, -1)\n",
        "    dark_img = cv2.imread(dark_img_path, -1)\n",
        "    logging.info(\"Done Loading Images..\")\n",
        "\n",
        "    logging.info(\"Applying Flat Field Correction..\")\n",
        "    imgFFC = ffcimg(proj_img, gain_img, dark_img)\n",
        "    logging.info(\"Done Applying Flat Field Correction..\")\n",
        "\n",
        "    logging.info(\"Applying Spatial Calibration Correction..\")\n",
        "    calib_spat_param_path = os.path.abspath(os.path.join(os.path.dirname(\"main.py\"), os.pardir, 'datacitra')) + os.sep + 'Kalibrasi' + os.sep + 'bed_44_35.npz'\n",
        "    # img_spatially_calibrated = calibrate_spasial(calib_spat_param_path,imgFFC)\n",
        "    logging.info(\"Done Applying Spatial Calibration Correction..\")\n",
        "\n",
        "    # plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    # plt.title('Citra Asli')\n",
        "    # plt.imshow(proj_img, cmap='gray')\n",
        "    # plt.axis('off')\n",
        "\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    # plt.title('Citra Median Filter')\n",
        "    # plt.imshow(img_spatially_calibrated, cmap='gray')\n",
        "    # plt.axis('off')\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "    logging.info(\"Start Image Decompositiion (BEMD)..\")\n",
        "    # Perform FABEMD decomposition\n",
        "    BIMFs = fabemd(cp.asarray(imgFFC), max_iterations=1, threshold=1, initial_window_size=32, local_extrema_count=10)\n",
        "    logging.info(\"Finish Image Decompositiion (BEMD)..\")\n",
        "\n",
        "    # Calculating Energies of BIMFs\n",
        "    energies = []\n",
        "    for bimf in BIMFs:\n",
        "        energy = np.sum(np.square(np.array(bimf.get())))\n",
        "        energies.append(energy)\n",
        "\n",
        "    logging.info(\"Finding Best Processed Image with Various Parameters..\")\n",
        "    # Find Best Processed Image\n",
        "    # Define parameter ranges\n",
        "    d0_values = [20, 30, 40]\n",
        "    rh_values = [1.5, 2.0, 2.5]\n",
        "    rl_values = [0.3, 0.5]\n",
        "    gamma_values = [0.8]\n",
        "    clip_limit_values = [3.0]\n",
        "    tile_grid_size_values = [(8, 8)]\n",
        "\n",
        "    # Generate parameter combinations\n",
        "    parameter_combinations = list(\n",
        "        itertools.product(\n",
        "            d0_values,\n",
        "            rh_values,\n",
        "            rl_values,\n",
        "            gamma_values,\n",
        "            clip_limit_values,\n",
        "            tile_grid_size_values,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(\"Total parameter combinations:\", len(parameter_combinations))\n",
        "\n",
        "    # Initialize variables to store evaluation results\n",
        "    best_cii = 0.0\n",
        "    best_entropy = 0.0\n",
        "    best_emr = 0.0\n",
        "    best_parameters = None\n",
        "    best_image = None\n",
        "\n",
        "    # Use ThreadPoolExecutor to process parameter combinations concurrently\n",
        "    with ThreadPoolExecutor(8) as executor:\n",
        "        futures = [executor.submit(process_parameters, params, imgFFC, BIMFs, energies ) for params in parameter_combinations]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            evaluation_result, cii, entropy, eme, params, clahe_image = future.result()\n",
        "            print(f\"params {params}, score {evaluation_result}, cii {cii}, ent {entropy}, eme {eme}\")\n",
        "            if evaluation_result > best_cii + best_entropy + best_emr:\n",
        "                best_cii = cii\n",
        "                best_entropy = entropy\n",
        "                best_emr = eme\n",
        "                best_parameters = params\n",
        "                best_image = clahe_image\n",
        "    min_val = cp.min(best_image)\n",
        "    max_val = cp.max(best_image)\n",
        "    best_image = min_val + ((best_image - min_val) / (max_val - min_val) * 65535)\n",
        "    best_image = ir.resize_image(best_image, 4096).get()\n",
        "    logging.info(f\"best_image data type : {type(best_image)}\")\n",
        "    # Save best image\n",
        "    filename = Path(proj_img_path).stem\n",
        "    filename = filename + \"_processed.tiff\"\n",
        "    cv2.imwrite(datacitra_path+ os.sep + filename, best_image.astype(np.uint16),params=(cv2.IMWRITE_TIFF_COMPRESSION, 1))\n",
        "    print(best_parameters)\n",
        "    logging.info(\"Best Processed Image Acquired and saved..\")\n",
        "\n",
        "    # plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    # plt.title('Citra Asli')\n",
        "    # plt.imshow(proj_img, cmap='gray')\n",
        "    # plt.axis('off')\n",
        "\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    # plt.title('Citra Median Filter')\n",
        "    # plt.imshow(img_spatially_calibrated, cmap='gray')\n",
        "    # plt.axis('off')\n",
        "\n",
        "    # plt.figure(figsize=(12, 6))\n",
        "    # plt.subplot(1, 3, 1)\n",
        "    # plt.imshow(img_spatially_calibrated, cmap=\"gray\")\n",
        "    # plt.title(\"Original Image\")\n",
        "    # plt.axis(\"off\")\n",
        "\n",
        "    # plt.subplot(1, 3, 2)\n",
        "    # plt.imshow(residue, cmap=\"gray\")\n",
        "    # plt.title(\"Residue\")\n",
        "    # plt.axis(\"off\")\n",
        "\n",
        "    # plt.subplot(1, 3, 3)\n",
        "    # plt.imshow(BIMFs[len(BIMFs)-1].get(),cmap=\"gray\")\n",
        "    # plt.title(f\"BIMF {len(BIMFs)}\")\n",
        "    # plt.axis(\"off\")\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title('Citra Asli')\n",
        "    plt.imshow(img_spatially_calibrated, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title('Citra Terbaik')\n",
        "    plt.imshow(best_image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    logging.info(\"Cleaning Up Memory..\")\n",
        "    del datacitra_path, proj_img_path, gain_img_path, dark_img_path\n",
        "    del proj_img, gain_img, dark_img\n",
        "    del imgFFC, calib_spat_param_path, img_spatially_calibrated\n",
        "    del BIMFs, energies\n",
        "    del d0_values, rh_values, rl_values, gamma_values, clip_limit_values, tile_grid_size_values\n",
        "    del parameter_combinations, best_cii, best_emr, best_entropy, best_parameters, best_image\n",
        "    del min_val, max_val\n",
        "    cp._default_memory_pool.free_all_blocks()\n",
        "    gc.collect()\n",
        "    logging.info(\"Memory Cleaned..\")\n",
        "\n",
        "def processing_image_pipeline(proj_img, gain_img, dark_img, calib_params):\n",
        "    logging.info(\"Applying Flat Field Correction..\")\n",
        "    imgFFC = ffcimg(proj_img, gain_img, dark_img)  # Replace with your actual FFC function\n",
        "    logging.info(\"Done Applying Flat Field Correction..\")\n",
        "\n",
        "    logging.info(\"Applying Spatial Calibration Correction..\")\n",
        "    calib_spat_param_path = \"path_to_calibration_file.npz\"  # Replace with your calibration file path\n",
        "    img_spatially_calibrated = calibrate_spasial(calib_params, imgFFC)  # Uncomment if implemented\n",
        "    # img_spatially_calibrated = imgFFC  # Placeholder\n",
        "    logging.info(\"Done Applying Spatial Calibration Correction..\")\n",
        "\n",
        "    logging.info(\"Start Image Decomposition (BEMD)..\")\n",
        "    BIMFs = fabemd(cp.asarray(imgFFC), max_iterations=1, threshold=1, initial_window_size=32, local_extrema_count=10)  # Replace with your actual FABEMD function\n",
        "    logging.info(\"Finish Image Decomposition (BEMD)..\")\n",
        "\n",
        "    # Calculating Energies of BIMFs\n",
        "    energies = []\n",
        "    for bimf in BIMFs:\n",
        "        energy = np.sum(np.square(np.array(bimf.get())))\n",
        "        energies.append(energy)\n",
        "\n",
        "    logging.info(\"Finding Best Processed Image with Various Parameters..\")\n",
        "    # Define parameter ranges\n",
        "    d0_values = [20, 30, 40]\n",
        "    rh_values = [1.5, 2.0, 2.5]\n",
        "    rl_values = [0.3, 0.5]\n",
        "    gamma_values = [0.8]\n",
        "    clip_limit_values = [3.0]\n",
        "    tile_grid_size_values = [(8, 8)]\n",
        "\n",
        "    # Generate parameter combinations\n",
        "    parameter_combinations = list(\n",
        "        itertools.product(\n",
        "            d0_values,\n",
        "            rh_values,\n",
        "            rl_values,\n",
        "            gamma_values,\n",
        "            clip_limit_values,\n",
        "            tile_grid_size_values,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Initialize variables to store evaluation results\n",
        "    best_cii = 0.0\n",
        "    best_entropy = 0.0\n",
        "    best_emr = 0.0\n",
        "    best_parameters = None\n",
        "    best_image = None\n",
        "\n",
        "    # Use ThreadPoolExecutor to process parameter combinations concurrently\n",
        "    with ThreadPoolExecutor(8) as executor:\n",
        "        futures = [executor.submit(process_parameters, params, imgFFC, BIMFs, energies) for params in parameter_combinations]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            evaluation_result, cii, entropy, eme, params, clahe_image = future.result()\n",
        "            if evaluation_result > best_cii + best_entropy + best_emr:\n",
        "                best_cii = cii\n",
        "                best_entropy = entropy\n",
        "                best_emr = eme\n",
        "                best_parameters = params\n",
        "                best_image = clahe_image\n",
        "\n",
        "    # Normalize and resize the best image\n",
        "    min_val = cp.min(best_image)\n",
        "    max_val = cp.max(best_image)\n",
        "    best_image = min_val + ((best_image - min_val) / (max_val - min_val) * 65535)\n",
        "    best_image = resize_image(best_image, 4096).get()  # Replace with your actual resize function\n",
        "\n",
        "    logging.info(\"Best Processed Image Acquired..\")\n",
        "\n",
        "    # Clean up memory\n",
        "    logging.info(\"Cleaning Up Memory..\")\n",
        "    del imgFFC, BIMFs, energies, parameter_combinations, best_cii, best_emr, best_entropy, best_parameters\n",
        "    cp._default_memory_pool.free_all_blocks()\n",
        "    gc.collect()\n",
        "    logging.info(\"Memory Cleaned..\")\n",
        "\n",
        "    return img_spatially_calibrated, best_image"
      ],
      "metadata": {
        "id": "Hpbymt8MBiLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxfvF9dsZOaj",
        "outputId": "1035ef4f-32d0-418f-d5a1-3c7ff55e646f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Import the processing pipeline\n",
        "# from your_script import processing_image_pipeline\n",
        "\n",
        "st.title(\"Image Processing Pipeline\")\n",
        "\n",
        "# Upload images\n",
        "proj_img_file = st.file_uploader(\"Upload Projection Image\")\n",
        "gain_img_file = st.file_uploader(\"Upload Gain Image\")\n",
        "dark_img_file = st.file_uploader(\"Upload Dark Image\")\n",
        "calib_file = st.file_uploader(\"Upload Calibration File (.npz)\", type=[\"npz\"])\n",
        "\n",
        "# Create a placeholder for the results\n",
        "result_placeholder = st.empty()\n",
        "\n",
        "if proj_img_file and gain_img_file and dark_img_file and calib_file:\n",
        "    # Read images\n",
        "    proj_img = np.array(Image.open(proj_img_file))\n",
        "    gain_img = np.array(Image.open(gain_img_file))\n",
        "    dark_img = np.array(Image.open(dark_img_file))\n",
        "\n",
        "    # Load calibration parameters from the .npz file\n",
        "    calib_params = np.load(calib_file)\n",
        "\n",
        "    # Process button\n",
        "    if st.button(\"Process\"):\n",
        "        # Process images\n",
        "        with st.spinner(\"Processing...\"):  # Display a spinner while processing\n",
        "            original_image, processed_image = processing_image_pipeline(proj_img, gain_img, dark_img, calib_params)\n",
        "\n",
        "        # Display results in the placeholder\n",
        "        result_placeholder.subheader(\"Results\")\n",
        "        result_placeholder.image([original_image, processed_image], caption=[\"Original Image\", \"Processed Image\"], use_column_width=True, clamp=True)\n",
        "else:\n",
        "    st.warning(\"Please upload all required files (images and calibration file) to proceed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3ptfgLbZFrU",
        "outputId": "2ffa627d-150d-4025-d07d-6cc064bc7fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "!./cloudflared tunnel --url http://localhost:8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1Q-fXSaZRzA",
        "outputId": "ecad1d41-0160-485d-cb6e-f141947410e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-03 09:10:35--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.4.2/cloudflared-linux-amd64 [following]\n",
            "--2025-05-03 09:10:35--  https://github.com/cloudflare/cloudflared/releases/download/2025.4.2/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/a6b2a67b-5629-4df3-aa0c-8146365a1d48?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250503%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250503T091035Z&X-Amz-Expires=300&X-Amz-Signature=8837c82b11912797f65313f68aea50c9a33cf5b36e70751dbe248ddc48c867ca&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-03 09:10:35--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/a6b2a67b-5629-4df3-aa0c-8146365a1d48?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250503%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250503T091035Z&X-Amz-Expires=300&X-Amz-Signature=8837c82b11912797f65313f68aea50c9a33cf5b36e70751dbe248ddc48c867ca&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37819582 (36M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared’\n",
            "\n",
            "cloudflared         100%[===================>]  36.07M   226MB/s    in 0.2s    \n",
            "\n",
            "2025-05-03 09:10:36 (226 MB/s) - ‘cloudflared’ saved [37819582/37819582]\n",
            "\n",
            "\u001b[90m2025-05-03T09:10:37Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-05-03T09:10:37Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m |  https://contribution-semiconductor-exceptions-perception.trycloudflare.com                |\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.4.2 (Checksum c4f2c09e38569f850da274d3b8502ea88304c1bd0a4f1528b420c23f715d4551)\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.22.10, GoArch: amd64\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 4259fdef-9b24-46f8-aa9f-07af38351c35\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m Using [CurveID(4588) CurveID(25497) CurveP256] as curve preferences \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77\n",
            "2025/05/03 09:10:41 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-05-03T09:10:41Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m4b7a30a8-0119-48da-9657-c57faaaf9015 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mlocation=\u001b[0mlax05 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 53 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 53 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://contribution-semiconductor-exceptions-perception.trycloudflare.com/static/media/SourceSansPro-SemiBold.sKQIyTMz.woff2 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 45 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 45 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://contribution-semiconductor-exceptions-perception.trycloudflare.com/static/js/index.BqDl3eRM.js \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 57 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 57 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://contribution-semiconductor-exceptions-perception.trycloudflare.com/static/media/SourceSansPro-Regular.DZLUzqI4.woff2 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 41 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 41 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://contribution-semiconductor-exceptions-perception.trycloudflare.com/static/media/SourceSansPro-Bold.-6c9oR8J.woff2 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 73 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 73 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://contribution-semiconductor-exceptions-perception.trycloudflare.com/static/media/SourceSansPro-SemiBold.sKQIyTMz.woff2 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 69 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 69 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://contribution-semiconductor-exceptions-perception.trycloudflare.com/static/js/index.BqDl3eRM.js \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 65 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 65 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://contribution-semiconductor-exceptions-perception.trycloudflare.com/static/media/SourceSansPro-Regular.DZLUzqI4.woff2 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 61 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-05-03T09:10:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 61 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://contribution-semiconductor-exceptions-perception.trycloudflare.com/static/media/SourceSansPro-Bold.-6c9oR8J.woff2 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-05-03T09:16:19Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}